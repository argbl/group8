#!/usr/bin/env python3
import rospy
from sensor_msgs.msg import Image
from std_msgs.msg import Bool
import cv2
from cv_bridge import CvBridge
import mediapipe as mp
import numpy as np

class CameraNode:
    def __init__(self):
        rospy.init_node('camera_node', anonymous=True)
        self.bridge = CvBridge()
        self.face_pub = rospy.Publisher('/person_detected', Bool, queue_size=10)
        self.gesture_pub = rospy.Publisher('/hand_gesture', Bool, queue_size=10)  # True=OK, False=Open Palm

        # äººè„¸æ£€æµ‹å™¨
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )

        # MediaPipe æ‰‹éƒ¨æ¨¡å‹
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(static_image_mode=False,
                                         max_num_hands=1,
                                         min_detection_confidence=0.5,
                                         min_tracking_confidence=0.5)

        rospy.Subscriber('/camera/color/image_raw', Image, self.image_callback)
        rospy.loginfo("ğŸ¥ CameraNode with Face + Hand detection started.")
        rospy.spin()

    def image_callback(self, msg):
        try:
            frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # äººè„¸æ£€æµ‹
            faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)
            self.face_pub.publish(Bool(data=len(faces) > 0))
            if len(faces) > 0:
                rospy.loginfo("ğŸ‘¤ Face detected")
                for (x, y, w, h) in faces:
                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

            # æ‰‹åŠ¿æ£€æµ‹
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            result = self.hands.process(rgb_frame)
            gesture_detected = None

            if result.multi_hand_landmarks:
                for hand_landmarks in result.multi_hand_landmarks:
                    gesture_detected = self.classify_gesture(hand_landmarks)
                    if gesture_detected is not None:
                        self.gesture_pub.publish(Bool(data=gesture_detected))
                        gesture_name = "OK" if gesture_detected else "Open Palm"
                        rospy.loginfo(f"âœ‹ Hand gesture: {gesture_name}")
                    mp.solutions.drawing_utils.draw_landmarks(
                        frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS
                    )

            # æ˜¾ç¤ºå›¾åƒ
            cv2.imshow("Camera - Face + Gesture Detection", frame)
            cv2.waitKey(1)

        except Exception as e:
            rospy.logerr(f"âŒ Error in image callback: {str(e)}")

    def classify_gesture(self, landmarks):
        """
        åˆ¤æ–­æ˜¯å¦ä¸º OK æˆ–å¼ å¼€æ‰‹æŒï¼š
        - OK: æ‹‡æŒ‡æŒ‡å°–é è¿‘é£ŸæŒ‡æŒ‡å°–
        - å¼ å¼€: æ‰‹æŒ‡éƒ½å¼ å¼€ï¼Œæ‹‡æŒ‡ä¸è´´è¿‘å…¶ä»–æŒ‡å°–
        """
        thumb_tip = landmarks.landmark[self.mp_hands.HandLandmark.THUMB_TIP]
        index_tip = landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]
        middle_tip = landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP]

        # è®¡ç®—æ‹‡æŒ‡å’Œé£ŸæŒ‡è·ç¦»
        dist_thumb_index = np.linalg.norm(np.array([thumb_tip.x, thumb_tip.y]) - np.array([index_tip.x, index_tip.y]))
        dist_thumb_middle = np.linalg.norm(np.array([thumb_tip.x, thumb_tip.y]) - np.array([middle_tip.x, middle_tip.y]))

        # OK: è·ç¦»å¾ˆå°ï¼ˆ0.05 æ˜¯ç»éªŒå€¼ï¼‰
        if dist_thumb_index < 0.05:
            return True
        # å¼ å¼€æ‰‹æŒï¼šæ‹‡æŒ‡è¿œç¦»ä¸­æŒ‡
        elif dist_thumb_middle > 0.1:
            return False
        else:
            return None  # æ— æ³•è¯†åˆ«

if __name__ == '__main__':
    try:
        CameraNode()
    except rospy.ROSInterruptException:
        pass
